{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18fa48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sneha\n",
      "[nltk_data]     Kadam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sneha\n",
      "[nltk_data]     Kadam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sneha Kadam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sneha\n",
      "[nltk_data]     Kadam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Sneha\n",
      "[nltk_data]     Kadam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk #Natural Language Toolkit\n",
    "nltk.download('punkt')#unsupervised trainable model\n",
    "nltk.download('wordnet')#is a lexical database of English.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')#words that frequently appear in any language\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651a4821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK Stemmers. Interfaces used to remove morphological affixes from words, leaving only the word stem\n",
    "from nltk.stem import PorterStemmer  #a process for removing suffixes from words in English.\n",
    "from nltk.stem import LancasterStemmer #hat means it gets a certain string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc03b0",
   "metadata": {},
   "source": [
    "# Reading from Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "593d789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_2 = open('big.txt', \"r\")\n",
    "with open('text3.txt', 'r') as file:\n",
    "    text_op = file.read().replace('\\n', '')\n",
    "# text_op = text_2.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e59769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058\n"
     ]
    }
   ],
   "source": [
    "print(len(text_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb534312",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='I am sneha kadam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b36e9",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d825eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contrary to popular belief, Lorem Ipsum is not simply random text.', 'It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old.', 'Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source.', 'Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC.', 'This book is a treatise on the theory of ethics, very popular during the Renaissance.', 'The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested.', 'Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.']\n"
     ]
    }
   ],
   "source": [
    "tokens_sents = nltk.sent_tokenize(text_op)\n",
    "print(tokens_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6db3da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contrary', 'to', 'popular', 'belief', ',', 'Lorem', 'Ipsum', 'is', 'not', 'simply', 'random', 'text', '.', 'It', 'has', 'roots', 'in', 'a', 'piece', 'of', 'classical', 'Latin', 'literature', 'from', '45', 'BC', ',', 'making', 'it', 'over', '2000', 'years', 'old', '.', 'Richard', 'McClintock', ',', 'a', 'Latin', 'professor', 'at', 'Hampden-Sydney', 'College', 'in', 'Virginia', ',', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'Latin', 'words', ',', 'consectetur', ',', 'from', 'a', 'Lorem', 'Ipsum', 'passage', ',', 'and', 'going', 'through', 'the', 'cites', 'of', 'the', 'word', 'in', 'classical', 'literature', ',', 'discovered', 'the', 'undoubtable', 'source', '.', 'Lorem', 'Ipsum', 'comes', 'from', 'sections', '1.10.32', 'and', '1.10.33', 'of', '``', 'de', 'Finibus', 'Bonorum', 'et', 'Malorum', \"''\", '(', 'The', 'Extremes', 'of', 'Good', 'and', 'Evil', ')', 'by', 'Cicero', ',', 'written', 'in', '45', 'BC', '.', 'This', 'book', 'is', 'a', 'treatise', 'on', 'the', 'theory', 'of', 'ethics', ',', 'very', 'popular', 'during', 'the', 'Renaissance', '.', 'The', 'first', 'line', 'of', 'Lorem', 'Ipsum', ',', '``', 'Lorem', 'ipsum', 'dolor', 'sit', 'amet', '..', \"''\", ',', 'comes', 'from', 'a', 'line', 'in', 'section', '1.10.32.The', 'standard', 'chunk', 'of', 'Lorem', 'Ipsum', 'used', 'since', 'the', '1500s', 'is', 'reproduced', 'below', 'for', 'those', 'interested', '.', 'Sections', '1.10.32', 'and', '1.10.33', 'from', '``', 'de', 'Finibus', 'Bonorum', 'et', 'Malorum', \"''\", 'by', 'Cicero', 'are', 'also', 'reproduced', 'in', 'their', 'exact', 'original', 'form', ',', 'accompanied', 'by', 'English', 'versions', 'from', 'the', '1914', 'translation', 'by', 'H.', 'Rackham', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_words = nltk.word_tokenize(text_op)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be46beb",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffd20815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is a text preprocessing technique used in natural language processing (NLP) to reduce words to their root or base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6adf8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contrari', 'to', 'popular', 'belief', ',', 'lorem', 'ipsum', 'is', 'not', 'simpli', 'random', 'text', '.', 'it', 'ha', 'root', 'in', 'a', 'piec', 'of', 'classic', 'latin', 'literatur', 'from', '45', 'bc', ',', 'make', 'it', 'over', '2000', 'year', 'old', '.', 'richard', 'mcclintock', ',', 'a', 'latin', 'professor', 'at', 'hampden-sydney', 'colleg', 'in', 'virginia', ',', 'look', 'up', 'one', 'of', 'the', 'more', 'obscur', 'latin', 'word', ',', 'consectetur', ',', 'from', 'a', 'lorem', 'ipsum', 'passag', ',', 'and', 'go', 'through', 'the', 'cite', 'of', 'the', 'word', 'in', 'classic', 'literatur', ',', 'discov', 'the', 'undoubt', 'sourc', '.', 'lorem', 'ipsum', 'come', 'from', 'section', '1.10.32', 'and', '1.10.33', 'of', '``', 'de', 'finibu', 'bonorum', 'et', 'malorum', \"''\", '(', 'the', 'extrem', 'of', 'good', 'and', 'evil', ')', 'by', 'cicero', ',', 'written', 'in', '45', 'bc', '.', 'thi', 'book', 'is', 'a', 'treatis', 'on', 'the', 'theori', 'of', 'ethic', ',', 'veri', 'popular', 'dure', 'the', 'renaiss', '.', 'the', 'first', 'line', 'of', 'lorem', 'ipsum', ',', '``', 'lorem', 'ipsum', 'dolor', 'sit', 'amet', '..', \"''\", ',', 'come', 'from', 'a', 'line', 'in', 'section', '1.10.32.the', 'standard', 'chunk', 'of', 'lorem', 'ipsum', 'use', 'sinc', 'the', '1500', 'is', 'reproduc', 'below', 'for', 'those', 'interest', '.', 'section', '1.10.32', 'and', '1.10.33', 'from', '``', 'de', 'finibu', 'bonorum', 'et', 'malorum', \"''\", 'by', 'cicero', 'are', 'also', 'reproduc', 'in', 'their', 'exact', 'origin', 'form', ',', 'accompani', 'by', 'english', 'version', 'from', 'the', '1914', 'translat', 'by', 'h.', 'rackham', '.']\n"
     ]
    }
   ],
   "source": [
    "stem=[]\n",
    "for i in tokens_words:\n",
    "    ps = PorterStemmer()\n",
    "    stem_word= ps.stem(i)\n",
    "    stem.append(stem_word)\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a7fad",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98cbdbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0231293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrari to popular belief , lorem ipsum is not simpli random text . it ha root in a piec of classic latin literatur from 45 bc , make it over 2000 year old . richard mcclintock , a latin professor at hampden-sydney colleg in virginia , look up one of the more obscur latin word , consectetur , from a lorem ipsum passag , and go through the cite of the word in classic literatur , discov the undoubt sourc . lorem ipsum come from section 1.10.32 and 1.10.33 of `` de finibu bonorum et malorum '' ( the extrem of good and evil ) by cicero , written in 45 bc . thi book is a treatis on the theori of ethic , veri popular dure the renaiss . the first line of lorem ipsum , `` lorem ipsum dolor sit amet .. '' , come from a line in section 1.10.32.the standard chunk of lorem ipsum use sinc the 1500 is reproduc below for those interest . section 1.10.32 and 1.10.33 from `` de finibu bonorum et malorum '' by cicero are also reproduc in their exact origin form , accompani by english version from the 1914 translat by h. rackham .\n"
     ]
    }
   ],
   "source": [
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in stem])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b1afaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contrari', 'to', 'popular', 'belief', ',', 'lorem', 'ipsum', 'is', 'not', 'simpli', 'random', 'text', '.', 'it', 'ha', 'root', 'in', 'a', 'piec', 'of', 'classic', 'latin', 'literatur', 'from', '45', 'bc', ',', 'make', 'it', 'over', '2000', 'year', 'old', '.', 'richard', 'mcclintock', ',', 'a', 'latin', 'professor', 'at', 'hampden-sydney', 'colleg', 'in', 'virginia', ',', 'look', 'up', 'one', 'of', 'the', 'more', 'obscur', 'latin', 'word', ',', 'consectetur', ',', 'from', 'a', 'lorem', 'ipsum', 'passag', ',', 'and', 'go', 'through', 'the', 'cite', 'of', 'the', 'word', 'in', 'classic', 'literatur', ',', 'discov', 'the', 'undoubt', 'sourc', '.', 'lorem', 'ipsum', 'come', 'from', 'section', '1.10.32', 'and', '1.10.33', 'of', '``', 'de', 'finibu', 'bonorum', 'et', 'malorum', \"''\", '(', 'the', 'extrem', 'of', 'good', 'and', 'evil', ')', 'by', 'cicero', ',', 'written', 'in', '45', 'bc', '.', 'thi', 'book', 'is', 'a', 'treatis', 'on', 'the', 'theori', 'of', 'ethic', ',', 'veri', 'popular', 'dure', 'the', 'renaiss', '.', 'the', 'first', 'line', 'of', 'lorem', 'ipsum', ',', '``', 'lorem', 'ipsum', 'dolor', 'sit', 'amet', '..', \"''\", ',', 'come', 'from', 'a', 'line', 'in', 'section', '1.10.32.the', 'standard', 'chunk', 'of', 'lorem', 'ipsum', 'use', 'sinc', 'the', '1500', 'is', 'reproduc', 'below', 'for', 'those', 'interest', '.', 'section', '1.10.32', 'and', '1.10.33', 'from', '``', 'de', 'finibu', 'bonorum', 'et', 'malorum', \"''\", 'by', 'cicero', 'are', 'also', 'reproduc', 'in', 'their', 'exact', 'origin', 'form', ',', 'accompani', 'by', 'english', 'version', 'from', 'the', '1914', 'translat', 'by', 'h.', 'rackham', '.']\n"
     ]
    }
   ],
   "source": [
    "leme=[]\n",
    "for i in stem:\n",
    "    lemetized_word=lemmatizer.lemmatize(i)\n",
    "    leme.append(lemetized_word)\n",
    "print(leme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5015e3d",
   "metadata": {},
   "source": [
    "# POS (Parts of Speech) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09e60b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of Speech:  [('contrari', 'NN'), ('to', 'TO'), ('popular', 'JJ'), ('belief', 'NN'), (',', ','), ('lorem', 'JJ'), ('ipsum', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('simpli', 'JJ'), ('random', 'JJ'), ('text', 'NN'), ('.', '.'), ('it', 'PRP'), ('ha', 'VBD'), ('root', 'NN'), ('in', 'IN'), ('a', 'DT'), ('piec', 'NN'), ('of', 'IN'), ('classic', 'JJ'), ('latin', 'JJ'), ('literatur', 'NN'), ('from', 'IN'), ('45', 'CD'), ('bc', 'NN'), (',', ','), ('make', 'VBP'), ('it', 'PRP'), ('over', 'IN'), ('2000', 'CD'), ('year', 'NN'), ('old', 'JJ'), ('.', '.'), ('richard', 'NN'), ('mcclintock', 'NN'), (',', ','), ('a', 'DT'), ('latin', 'NN'), ('professor', 'NN'), ('at', 'IN'), ('hampden-sydney', 'JJ'), ('colleg', 'NN'), ('in', 'IN'), ('virginia', 'NN'), (',', ','), ('look', 'VB'), ('up', 'RP'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('more', 'RBR'), ('obscur', 'JJ'), ('latin', 'JJ'), ('word', 'NN'), (',', ','), ('consectetur', 'NN'), (',', ','), ('from', 'IN'), ('a', 'DT'), ('lorem', 'JJ'), ('ipsum', 'NN'), ('passag', 'NN'), (',', ','), ('and', 'CC'), ('go', 'VB'), ('through', 'IN'), ('the', 'DT'), ('cite', 'NN'), ('of', 'IN'), ('the', 'DT'), ('word', 'NN'), ('in', 'IN'), ('classic', 'JJ'), ('literatur', 'NN'), (',', ','), ('discov', 'VBZ'), ('the', 'DT'), ('undoubt', 'JJ'), ('sourc', 'NN'), ('.', '.'), ('lorem', 'CC'), ('ipsum', 'JJ'), ('come', 'NN'), ('from', 'IN'), ('section', 'NN'), ('1.10.32', 'CD'), ('and', 'CC'), ('1.10.33', 'CD'), ('of', 'IN'), ('``', '``'), ('de', 'FW'), ('finibu', 'FW'), ('bonorum', 'NN'), ('et', 'FW'), ('malorum', 'NN'), (\"''\", \"''\"), ('(', '('), ('the', 'DT'), ('extrem', 'NN'), ('of', 'IN'), ('good', 'JJ'), ('and', 'CC'), ('evil', 'JJ'), (')', ')'), ('by', 'IN'), ('cicero', 'NN'), (',', ','), ('written', 'VBN'), ('in', 'IN'), ('45', 'CD'), ('bc', 'NN'), ('.', '.'), ('thi', 'JJ'), ('book', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('treatis', 'NN'), ('on', 'IN'), ('the', 'DT'), ('theori', 'NN'), ('of', 'IN'), ('ethic', 'JJ'), (',', ','), ('veri', 'JJ'), ('popular', 'JJ'), ('dure', 'NN'), ('the', 'DT'), ('renaiss', 'NN'), ('.', '.'), ('the', 'DT'), ('first', 'JJ'), ('line', 'NN'), ('of', 'IN'), ('lorem', 'NN'), ('ipsum', 'NN'), (',', ','), ('``', '``'), ('lorem', 'JJ'), ('ipsum', 'NN'), ('dolor', 'NN'), ('sit', 'NN'), ('amet', 'VBD'), ('..', 'NNP'), (\"''\", \"''\"), (',', ','), ('come', 'VB'), ('from', 'IN'), ('a', 'DT'), ('line', 'NN'), ('in', 'IN'), ('section', 'NN'), ('1.10.32.the', 'CD'), ('standard', 'JJ'), ('chunk', 'NN'), ('of', 'IN'), ('lorem', 'NN'), ('ipsum', 'NN'), ('use', 'NN'), ('sinc', 'VBD'), ('the', 'DT'), ('1500', 'CD'), ('is', 'VBZ'), ('reproduc', 'VBN'), ('below', 'IN'), ('for', 'IN'), ('those', 'DT'), ('interest', 'NN'), ('.', '.'), ('section', 'NN'), ('1.10.32', 'CD'), ('and', 'CC'), ('1.10.33', 'CD'), ('from', 'IN'), ('``', '``'), ('de', 'FW'), ('finibu', 'FW'), ('bonorum', 'NN'), ('et', 'FW'), ('malorum', 'NN'), (\"''\", \"''\"), ('by', 'IN'), ('cicero', 'NN'), ('are', 'VBP'), ('also', 'RB'), ('reproduc', 'VBN'), ('in', 'IN'), ('their', 'PRP$'), ('exact', 'JJ'), ('origin', 'JJ'), ('form', 'NN'), (',', ','), ('accompani', 'VBN'), ('by', 'IN'), ('english', 'JJ'), ('version', 'NN'), ('from', 'IN'), ('the', 'DT'), ('1914', 'CD'), ('translat', 'NN'), ('by', 'IN'), ('h.', 'NN'), ('rackham', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Parts of Speech: \",nltk.pos_tag(leme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7079f",
   "metadata": {},
   "source": [
    "# Stopwords in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f59f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "sw_nltk = stopwords.words('english')\n",
    "print(sw_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f4e4a",
   "metadata": {},
   "source": [
    "# Words that aren't stopwords in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd9801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [word for word in text.split() if word.lower() not in sw_nltk]\n",
    "# new_text = \" \".join(words)\n",
    "# print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1138fa9d",
   "metadata": {},
   "source": [
    "# Calculating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3ed3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_op = []\n",
    "with open('text3.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        text_op.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74cfd3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff93965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\\n', '\\n', 'The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.']\n"
     ]
    }
   ],
   "source": [
    "tr_idf_model  = TfidfVectorizer(stop_words='english')\n",
    "corpus = text_op\n",
    "tf_idf_vector = tr_idf_model.fit_transform(corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "041fd272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(3, 76)\n"
     ]
    }
   ],
   "source": [
    "print(type(tf_idf_vector))\n",
    "print(tf_idf_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6965066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20872435 0.         0.         0.09148248 0.13914957 0.06957478\n",
      "  0.18296496 0.         0.09148248 0.18296496 0.09148248 0.06957478\n",
      "  0.09148248 0.         0.06957478 0.09148248 0.18296496 0.09148248\n",
      "  0.18296496 0.09148248 0.09148248 0.09148248 0.09148248 0.\n",
      "  0.06957478 0.09148248 0.09148248 0.         0.09148248 0.06957478\n",
      "  0.         0.09148248 0.09148248 0.09148248 0.         0.34787392\n",
      "  0.27444744 0.18296496 0.18296496 0.09148248 0.34787392 0.09148248\n",
      "  0.06957478 0.09148248 0.09148248 0.09148248 0.         0.09148248\n",
      "  0.09148248 0.18296496 0.09148248 0.         0.09148248 0.09148248\n",
      "  0.         0.09148248 0.09148248 0.09148248 0.06957478 0.09148248\n",
      "  0.09148248 0.09148248 0.         0.09148248 0.09148248 0.09148248\n",
      "  0.         0.09148248 0.09148248 0.         0.         0.09148248\n",
      "  0.09148248 0.09148248 0.09148248 0.09148248]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.29774448 0.1957491  0.1957491  0.         0.14887224 0.14887224\n",
      "  0.         0.1957491  0.         0.         0.         0.14887224\n",
      "  0.         0.1957491  0.14887224 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.1957491\n",
      "  0.14887224 0.         0.         0.1957491  0.         0.14887224\n",
      "  0.1957491  0.         0.         0.         0.1957491  0.14887224\n",
      "  0.         0.         0.         0.         0.14887224 0.\n",
      "  0.14887224 0.         0.         0.         0.1957491  0.\n",
      "  0.         0.         0.         0.1957491  0.         0.\n",
      "  0.3914982  0.         0.         0.         0.14887224 0.\n",
      "  0.         0.         0.1957491  0.         0.         0.\n",
      "  0.1957491  0.         0.         0.1957491  0.1957491  0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "print(tf_idf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57fa5fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '1500s' '1914' '2000' '32' '33' '45' 'accompanied' 'amet' 'bc'\n",
      " 'belief' 'bonorum' 'book' 'chunk' 'cicero' 'cites' 'classical' 'college'\n",
      " 'comes' 'consectetur' 'contrary' 'discovered' 'dolor' 'english' 'et'\n",
      " 'ethics' 'evil' 'exact' 'extremes' 'finibus' 'form' 'going' 'good'\n",
      " 'hampden' 'interested' 'ipsum' 'latin' 'line' 'literature' 'looked'\n",
      " 'lorem' 'making' 'malorum' 'mcclintock' 'obscure' 'old' 'original'\n",
      " 'passage' 'piece' 'popular' 'professor' 'rackham' 'random' 'renaissance'\n",
      " 'reproduced' 'richard' 'roots' 'section' 'sections' 'simply' 'sit'\n",
      " 'source' 'standard' 'sydney' 'text' 'theory' 'translation' 'treatise'\n",
      " 'undoubtable' 'used' 'versions' 'virginia' 'word' 'words' 'written'\n",
      " 'years']\n"
     ]
    }
   ],
   "source": [
    "words_set = tr_idf_model.get_feature_names_out()\n",
    "print(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7313b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>1500s</th>\n",
       "      <th>1914</th>\n",
       "      <th>2000</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>45</th>\n",
       "      <th>accompanied</th>\n",
       "      <th>amet</th>\n",
       "      <th>bc</th>\n",
       "      <th>...</th>\n",
       "      <th>translation</th>\n",
       "      <th>treatise</th>\n",
       "      <th>undoubtable</th>\n",
       "      <th>used</th>\n",
       "      <th>versions</th>\n",
       "      <th>virginia</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.168823</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.096007</td>\n",
       "      <td>0.072816</td>\n",
       "      <td>0.060988</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.060988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>0.065250</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.030494</td>\n",
       "      <td>0.030494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.152830</td>\n",
       "      <td>0.113016</td>\n",
       "      <td>0.113016</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.083287</td>\n",
       "      <td>0.074489</td>\n",
       "      <td>0.105635</td>\n",
       "      <td>0.113016</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.105635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113016</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.113016</td>\n",
       "      <td>0.113016</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.052817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.104362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069575</td>\n",
       "      <td>0.034787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.208724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139150</td>\n",
       "      <td>0.069575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.253234</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.144011</td>\n",
       "      <td>0.109224</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.097875</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.045741</td>\n",
       "      <td>0.045741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.297744</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.148872</td>\n",
       "      <td>0.148872</td>\n",
       "      <td>0.182965</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.182965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             10     1500s      1914      2000        32        33        45  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.168823  0.065250  0.065250  0.030494  0.096007  0.072816  0.060988   \n",
       "std    0.152830  0.113016  0.113016  0.052817  0.083287  0.074489  0.105635   \n",
       "min    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25%    0.104362  0.000000  0.000000  0.000000  0.069575  0.034787  0.000000   \n",
       "50%    0.208724  0.000000  0.000000  0.000000  0.139150  0.069575  0.000000   \n",
       "75%    0.253234  0.097875  0.097875  0.045741  0.144011  0.109224  0.091482   \n",
       "max    0.297744  0.195749  0.195749  0.091482  0.148872  0.148872  0.182965   \n",
       "\n",
       "       accompanied      amet        bc  ...  translation  treatise  \\\n",
       "count     3.000000  3.000000  3.000000  ...     3.000000  3.000000   \n",
       "mean      0.065250  0.030494  0.060988  ...     0.065250  0.030494   \n",
       "std       0.113016  0.052817  0.105635  ...     0.113016  0.052817   \n",
       "min       0.000000  0.000000  0.000000  ...     0.000000  0.000000   \n",
       "25%       0.000000  0.000000  0.000000  ...     0.000000  0.000000   \n",
       "50%       0.000000  0.000000  0.000000  ...     0.000000  0.000000   \n",
       "75%       0.097875  0.045741  0.091482  ...     0.097875  0.045741   \n",
       "max       0.195749  0.091482  0.182965  ...     0.195749  0.091482   \n",
       "\n",
       "       undoubtable      used  versions  virginia      word     words  \\\n",
       "count     3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean      0.030494  0.065250  0.065250  0.030494  0.030494  0.030494   \n",
       "std       0.052817  0.113016  0.113016  0.052817  0.052817  0.052817   \n",
       "min       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25%       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "50%       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "75%       0.045741  0.097875  0.097875  0.045741  0.045741  0.045741   \n",
       "max       0.091482  0.195749  0.195749  0.091482  0.091482  0.091482   \n",
       "\n",
       "        written     years  \n",
       "count  3.000000  3.000000  \n",
       "mean   0.030494  0.030494  \n",
       "std    0.052817  0.052817  \n",
       "min    0.000000  0.000000  \n",
       "25%    0.000000  0.000000  \n",
       "50%    0.000000  0.000000  \n",
       "75%    0.045741  0.045741  \n",
       "max    0.091482  0.091482  \n",
       "\n",
       "[8 rows x 76 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set)\n",
    "\n",
    "df_tf_idf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd11281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>1500s</th>\n",
       "      <th>1914</th>\n",
       "      <th>2000</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>45</th>\n",
       "      <th>accompanied</th>\n",
       "      <th>amet</th>\n",
       "      <th>bc</th>\n",
       "      <th>...</th>\n",
       "      <th>translation</th>\n",
       "      <th>treatise</th>\n",
       "      <th>undoubtable</th>\n",
       "      <th>used</th>\n",
       "      <th>versions</th>\n",
       "      <th>virginia</th>\n",
       "      <th>word</th>\n",
       "      <th>words</th>\n",
       "      <th>written</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.208724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.139150</td>\n",
       "      <td>0.069575</td>\n",
       "      <td>0.182965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.182965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "      <td>0.091482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.297744</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148872</td>\n",
       "      <td>0.148872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.195749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         10     1500s      1914      2000        32        33        45  \\\n",
       "0  0.208724  0.000000  0.000000  0.091482  0.139150  0.069575  0.182965   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.297744  0.195749  0.195749  0.000000  0.148872  0.148872  0.000000   \n",
       "\n",
       "   accompanied      amet        bc  ...  translation  treatise  undoubtable  \\\n",
       "0     0.000000  0.091482  0.182965  ...     0.000000  0.091482     0.091482   \n",
       "1     0.000000  0.000000  0.000000  ...     0.000000  0.000000     0.000000   \n",
       "2     0.195749  0.000000  0.000000  ...     0.195749  0.000000     0.000000   \n",
       "\n",
       "       used  versions  virginia      word     words   written     years  \n",
       "0  0.000000  0.000000  0.091482  0.091482  0.091482  0.091482  0.091482  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.195749  0.195749  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 76 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d492c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803719d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa016211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
